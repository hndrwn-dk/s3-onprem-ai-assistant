# âš¡ S3 On-Prem AI Assistant - Speed Optimized

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/)
[![LangChain](https://img.shields.io/badge/LangChain-0.1%2B-green.svg)](https://python.langchain.com/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.29%2B-red)](https://streamlit.io/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.108%2B-teal)](https://fastapi.tiangolo.com/)
[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Offline AI](https://img.shields.io/badge/Offline-AI-important.svg)](https://ollama.com/)
[![PHI3 Powered](https://img.shields.io/badge/LLM-PHI3_Mini-ff69b4.svg)](https://ollama.com/library/phi3)
[![Performance](https://img.shields.io/badge/Performance-15--60x_Faster-brightgreen.svg)](#performance-improvements)

A **lightning-fast**, fully offline-capable AI assistant for answering operational, admin, and troubleshooting questions for on-premises S3-compatible platforms. **15-60x faster** than typical implementations with advanced caching and optimization.

## â˜• Support Me

If you find this project helpful, you can support me here:

[![Buy Me a Coffee](https://img.shields.io/badge/Buy%20Me%20a%20Coffee-yellow?style=for-the-badge&logo=buymeacoffee&logoColor=white)](https://buymeacoffee.com/hendrawan)


## ğŸ¢ Supported Platforms

- ğŸ“¦ **Cloudian HyperStore**
- ğŸ§Š **Huawei OceanStor** 
- âš¡ **Pure FlashBlade**
- ğŸ¢ **IBM Cloud Object Storage**
- ğŸ³ **MinIO**
- ğŸŒ **Dell ECS**
- ğŸ“Š **NetApp StorageGRID**
- And more S3-compatible storage systems

## âš¡ Key Features

- ğŸš€ **Lightning Fast**: Subsecond responses with multi-tier caching
- ğŸ§  **Smart Search**: Vector + pre-indexed + fallback search
- ğŸ“± **Multi-Interface**: CLI, Web UI, and REST API
- ğŸ”’ **Fully Offline**: No cloud dependency, runs entirely on-premises
- ğŸ“Š **Performance Monitoring**: Built-in timing and metrics
- ğŸ¯ **Intelligent Fallbacks**: Progressive search strategy
- ğŸ’¾ **Response Caching**: Instant answers for repeated queries
- ğŸ” **Document Types**: PDF, TXT, JSON, MD support

## ğŸš€ Performance Improvements

| Query Type | Before | After | Improvement |
|-----------|--------|--------|-------------|
| **Cached queries** | 3-5s | 0.01s | **300-500x faster** |
| **Bucket dept queries** | 5-10s | 0.1-0.5s | **20-100x faster** |
| **Vector search** | 5-15s | 1-3s | **5-10x faster** |
| **Model loading** | 5-10s | 2-5s | **2-3x faster** |

**Overall: 15-60x faster responses!** ğŸš€

---

## ğŸ“ Project Structure (Visual)

```bash
s3_onprem_ai_assistant/
â”œâ”€â”€ ğŸ”§ Core Files
â”‚   â”œâ”€â”€ config.py               # Optimized settings
â”‚   â”œâ”€â”€ model_cache.py          # Model caching system
â”‚   â”œâ”€â”€ response_cache.py       # Response caching
â”‚   â”œâ”€â”€ bucket_index.py         # Fast bucket search
â”‚   â””â”€â”€ utils.py                # Optimized utilities
â”œâ”€â”€ ğŸ—ï¸ Build & Processing
â”‚   â””â”€â”€ build_embeddings_all.py # Optimized embedding builder
â”œâ”€â”€ ğŸ–¥ï¸ User Interfaces
â”‚   â”œâ”€â”€ s3ai_query.py           # Ultra-fast CLI
â”‚   â”œâ”€â”€ api.py                  # Lightning-fast API
â”‚   â””â”€â”€ streamlit_ui.py         # Ultra-fast Web UI
â”œâ”€â”€ ğŸ“„ Documentation
â”‚   â””â”€â”€ docs/                   # Place your files here
â”‚       â”œâ”€â”€ *.pdf               # Admin guides
â”‚       â”œâ”€â”€ *.json              # Metadata files
â”‚       â”œâ”€â”€ *.txt               # Text documents
â”‚       â””â”€â”€ *.md                # Markdown files
â””â”€â”€ ğŸ—‚ï¸ Auto-generated
    â”œâ”€â”€ cache/                  # Response cache
    â”œâ”€â”€ s3_all_docs/           # Vector store
    â””â”€â”€ s3_all_chunks.pkl      # Document chunks
```

---

## ğŸ”§ Requirements

- **Python**: 3.8+ (Recommended: 3.9-3.11)
- **RAM**: Minimum 8GB (Recommended: 16GB+)
- **Storage**: 5GB+ for models and cache
- **Ollama**: For local LLM inference

---

## ğŸš€ Quick Start

### 1ï¸âƒ£ Clone and Setup

```bash
git clone https://github.com/hndrwn-dk/s3-onprem-ai-assistant.git
cd s3-onprem-ai-assistant

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2ï¸âƒ£ Install Ollama + Fast Model

```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Pull the optimized model (much faster than mistral)
ollama pull phi3:mini
```

### 3ï¸âƒ£ Prepare Your Documents

```bash
# Place your files in docs/
docs/
â”œâ”€â”€ cloudian_admin_guide.pdf
â”œâ”€â”€ minio_operations_manual.pdf
â”œâ”€â”€ sample_bucket_metadata_converted.txt
â””â”€â”€ other_documents...
```

### 4ï¸âƒ£ Build Optimized Index

```bash
# Build vector embeddings with optimization
python build_embeddings_all.py
```

### 5ï¸âƒ£ Start Querying!

```bash
# Ultra-fast CLI (recommended for speed)
python s3ai_query.py "show all buckets under dept: engineering"

# Web UI with progress indicators
streamlit run streamlit_ui.py

# REST API
python api.py
# Visit: http://localhost:8000/docs
```

---

## ğŸ§  Advanced Search Architecture

### Multi-Tier Speed Strategy

```
Query â†’ Cache Check â†’ Quick Bucket Search â†’ Vector Search â†’ Text Fallback
  â†“         â†“              â†“                 â†“             â†“
0.01s    0.1-0.5s       1-3s              2-5s        Last Resort
```

### 1ï¸âƒ£ **Cache Layer** (Instant)
- Response caching with TTL
- Instant answers for repeated queries
- Automatic cache management

### 2ï¸âƒ£ **Quick Bucket Search** (0.1-0.5s)
- Pre-indexed department/label searches
- Pattern matching for common queries
- Optimized for bucket metadata

### 3ï¸âƒ£ **Vector Search** (1-3s)
- FAISS-powered semantic search
- Optimized chunking strategy
- Reduced search parameters

### 4ï¸âƒ£ **Text Fallback** (2-5s)
- Direct text matching
- Context-aware responses
- Comprehensive coverage

---

## ğŸ’¬ Example Queries

| Category | Query | Expected Response Time |
|----------|-------|----------------------|
| ğŸ” **Bucket Lookup** | `"show all buckets under dept: engineering"` | **0.1-0.5s** |
| ğŸ·ï¸ **Label Search** | `"find buckets with label: backup"` | **0.1-0.5s** |
| ğŸ”§ **Troubleshooting** | `"How to fix S3 error code 403?"` | **1-3s** |
| ğŸ“Š **Metadata** | `"Show requestor_email for Bucket-001"` | **0.1-0.5s** |
| ğŸ› ï¸ **Admin Tasks** | `"How to purge versioned bucket in Cloudian?"` | **1-3s** |
| ğŸ”„ **Cached Queries** | Any previously asked question | **0.01s** |

---

## ğŸ”Œ API Usage

### REST API Examples

```bash
# Health check
curl -X GET "http://localhost:8000/health"

# Query with performance metrics
curl -X POST "http://localhost:8000/ask" \
  -H "Content-Type: application/json" \
  -d '{"question": "show all buckets under dept: engineering"}'

# Response includes timing
{
  "answer": "Here are the buckets under dept: engineering...",
  "source": "quick_search",
  "response_time": 0.23
}
```

### Postman Collection

```json
{
  "info": {
    "name": "S3 On-Prem AI Assistant v2.2.6 - Speed Optimized",
    "description": "Lightning-fast queries with performance metrics"
  },
  "item": [
    {
      "name": "Fast Department Search",
      "request": {
        "method": "POST",
        "header": [{"key": "Content-Type", "value": "application/json"}],
        "body": {
          "mode": "raw",
          "raw": "{\"question\": \"show all buckets under dept: engineering\"}"
        },
        "url": "http://localhost:8000/ask"
      }
    },
    {
      "name": "Performance Health Check",
      "request": {
        "method": "GET",
        "url": "http://localhost:8000/health"
      }
    }
  ]
}
```

---

## ğŸ–¥ï¸ Web UI Features

### Performance Dashboard
- âš¡ **Real-time timing metrics**
- ğŸ“Š **Performance indicators**
- ğŸ”„ **Progress tracking**
- ğŸ“ˆ **Response source tracking**

### User Experience
- ğŸ¯ **Instant cache hits**
- ğŸ” **Progressive search indicators**
- ğŸ“± **Responsive design**
- ğŸ—‚ï¸ **Query history**

---

## ğŸ”§ Configuration

### Performance Settings (`config.py`)

```python
# Speed optimizations
VECTOR_SEARCH_K = 3      # Reduced from 5 for speed
CHUNK_SIZE = 800         # Optimized chunk size
CHUNK_OVERLAP = 100      # Optimized overlap
CACHE_TTL_HOURS = 24     # Response cache TTL
```

### Model Configuration

```python
# Fast model settings
MODEL = "phi3:mini"      # Much faster than mistral
TEMPERATURE = 0.3        # Balanced creativity/speed
NUM_PREDICT = 512        # Limit response length
```

---

## ğŸ§ª Performance Monitoring

### Built-in Timing

```python
# Automatic timing decorators
@timing_decorator
def your_function():
    # Function automatically timed
    pass
```

### Performance Metrics

- ğŸ“Š **Model load times**
- âš¡ **Query response times**
- ğŸ¯ **Cache hit rates**
- ğŸ“ˆ **Search tier usage**

---

## ğŸ“‹ Troubleshooting

### Common Issues

1. **Slow first query**: Models loading (normal, cached afterward)
2. **No phi3:mini model**: Run `ollama pull phi3:mini`
3. **Empty responses**: Rebuild embeddings with `python build_embeddings_all.py`
4. **Memory issues**: Reduce `CHUNK_SIZE` in config.py

### Performance Optimization

```bash
# Clear cache for fresh start
rm -rf cache/

# Rebuild optimized embeddings
python build_embeddings_all.py

# Check model cache status
python -c "from model_cache import ModelCache; print(ModelCache.get_load_times())"
```

---

## ğŸ¯ Version History

### v2.2.6 - Speed Optimized (Current)
- âš¡ **15-60x performance improvement**
- ğŸ§  **Multi-tier caching system**
- ğŸ” **Pre-indexed bucket search**
- ğŸ“Š **Performance monitoring**
- ğŸš€ **PHI3-mini model support**

### v2.2.4 - Enhanced
- ğŸ“„ **Multi-format document support**
- ğŸ”„ **Improved fallback system**
- ğŸ› **Bug fixes and stability**

---

## ğŸ§ª Tested Environment

- âœ… **Python 3.8-3.11**
- âœ… **Ubuntu 20.04+ / CentOS 7+ / Windows 10+**
- âœ… **LangChain 0.1+**
- âœ… **Streamlit 1.29+**
- âœ… **FastAPI 0.108+**
- âœ… **Ollama (PHI3-mini)**

---

## ğŸ“˜ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ¤ Contributing

We welcome contributions! Areas of interest:

- ğŸ¢ **Additional S3 vendor support**
- âš¡ **Performance optimizations**
- ğŸ§  **New AI models**
- ğŸ“Š **Enhanced monitoring**
- ğŸ” **Better search algorithms**

### How to Contribute

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

---


<div align="center">

**âš¡ Built for Speed â€¢ ğŸ”’ Privacy-First â€¢ ğŸš€ Production-Ready**

*Made with â¤ï¸ for the S3 storage community*

</div>